{
  "hash": "d3d77445cfc5a3cbdcbd656b5ca06888",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Unlocking Middle Earth\"\nsubtitle: \"A RAG-based tool for exploring *The Lord of the Rings*\"\nauthor: \"Brent Benson\"\ndate: \"2024-06-19\"\ncategories: [technology, ai, books]\nimage: \"lotr_q_and_a.png\"\nexecute:\n  eval: false\n---\n\nReaders of J.R.R. Tolkien's books and watchers of the movie\nadaptations develop varying levels of understanding of the complex\ncharacters, geography, and lore of Middle Earth. Some dive in, head\nfirst, poring over the driest sections of *The Silmarillion* and other\nrelated texts and writings, while others enjoy the stories in a more\ntransactional context, understanding only what is needed to follow the\nstory.\n\nFor those looking for a more in-depth experience a discussion with a\nwell-versed Tolkien expert or reading along with others in a book club\ncan be an avenue towards deeper understanding and enjoyment. But we\ndon't always have someone around that fits the bill as a discussion\npartner.\n\nMy son has recently started delving into *The Lord of the Rings* again\nand it gave me the idea of using Generative AI as a tool for enhancing\nthe Tolkien experience.\n\n![An example of a question and answer from the RAG-based chatbot.](lotr_q_and_a.png)\n\nThis is an example of a question I asked after indexing the textual\ncontent of *The Lord of the Rings* books and making them available\nthrough a custom conversational retrieval chatbot using\nRetrieval-Augmented Generation.\n\n## Retrieval-Augmented Generation\n\nI am a big fan of using Retrieval-Augmented Generation or RAG as a way\nof using Large Language Models to interact, summarize, and answer\nquestions about a set of texts. In my work in technology and learning\nat Harvard Business School we have been indexing the textual elements\nof our active, social, case-based online business courses to create\ncourse assistant chatbots and interactive teaching elements.\n\nThe basic gist of RAG is to divide up the source text into a set of\nchunks that are then indexed using vector embeddings, creating a\nnumeric vector for each textual chunk that represents (at some level)\nthe semantics of the text. The chunks and associated This database can\nthen be used to find a set of documents related to a query or\nconversation that can be passed as context to a Large Language Model\n(LLM) to synthesize an answer.\n\nThe advantage of using RAG compared to using an LLM like ChatGPT\nwithout RAG, is that it focuses the conversation directly on the text,\nminimizes bias and hallucinations, and also provides the ability to\nshow direct references and links to the textual chunks used to create\nthe LLM response. The current architecture of LLMs does not allow them\nto provide direct references to source materials.\n\n## Indexing the text of *The Lord of the Rings*\n\nI used an ePub version of *The Lord of the Rings* that included all\nthree volumes and 6 books along with appendices. The custom chunking\ncusprogram (see <a href=\"#sec-chunking\"\nclass=\"quarto-xref\">Appendix</a>) produces a `JSONL` file with each\nline containing a chunk of text and associated metadata like this:\n\n\n```{json}\n{'class': 'appendix',\n 'id': 'appe-1',\n 'index': 991,\n 'label': 'APPENDIX A: ANNALS OF THE KINGS AND RULERS',\n 'page': 'page1071',\n 'playorder': '79',\n 'source': 'LordoftheRings_appe-1.html',\n 'text': '.\\n'\n         'After the fall of Sauron, Gimli brought south a part of the '\n         'Dwarf-folk of Erebor, and he became Lord of the Glittering Caves.\\r\\n'\n         '         He and his people did great works in Gondor and Rohan. For '\n         'Minas Tirith they forged gates of mithril and steel to replace those '\n         'broken by the Witch-king. Legolas his friend also brought south '\n         'Elves out of Greenwood, and they\\r\\n'\n         '         dwelt in Ithilien, and it became once again the fairest '\n         'country in all the westlands.\\n'\n         'But when King Elessar gave up his life Legolas followed at last the '\n         'desire of his heart and sailed over Sea.',\n 'title': 'The Lord of the Rings'}\n```\n\n\n## Appendix: Custom chunking program {#sec-chunking}\n\n::: {#a56d8024 .cell execution_count=1}\n``` {.python .cell-code}\nimport jsonlines\nfrom itertools import groupby\nfrom operator import itemgetter\nfrom ebooklib import epub, ITEM_DOCUMENT, ITEM_NAVIGATION\nfrom bs4 import BeautifulSoup\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nMAX_CHUNK_CHARS = 4000\n\ndef main():\n    epub_file = \"The_Lord_of_the_Rings.epub\"\n    jsonl_file = \"The_Lord_of_the_Rings.jsonl\"\n    print(f\"Process {epub_file}...\")\n    chunks = epub_text(epub_file)\n    print(f\"Identified {len(chunks)} textual elements.\")\n    for ix, chunk in enumerate(chunks):\n        chunk[\"index\"] = ix\n        chunk[\"title\"] = \"The Lord of the Rings\"\n        vol_book = path_to_volume_book(chunk[\"path\"])\n        if vol_book:\n            chunk.update(vol_book)\n        del chunk[\"path\"]\n    print(f\"Writing {jsonl_file}...\")\n    with jsonlines.open(jsonl_file, \"w\") as out_file:\n        out_file.write_all(chunks)\n    print(\"Done.\")\n\ndef epub_text(epub_file):\n    book = epub.read_epub(epub_file)\n    toc = table_of_contents(book)\n    contents = []\n    for source in toc:\n        node = toc[source]\n        item = book.get_item_with_href(source)\n        chunks = chapter_contents(item, node)\n        page_chunks = coalesce_pages(chunks)\n        contents.extend(page_chunks)\n    return contents\n\ndef table_of_contents(book):\n    nav_items = book.get_items_of_type(ITEM_NAVIGATION)\n    nav_item = next(nav_items)\n    ncx = BeautifulSoup(nav_item.get_content(), \"html.parser\")\n    np_nodes = []\n    for np in ncx.find(\"navmap\").find_all(\"navpoint\", recursive=False):\n        nodes = process_navpoint(np)\n        np_nodes.extend(nodes)\n    toc = {}\n    for node in np_nodes:\n        toc[node[\"source\"]] = node\n    return toc\n\ndef process_navpoint(navpoint, path=[]):\n    node = {\n        \"source\": navpoint.content[\"src\"],\n        \"label\": navpoint.find(\"navlabel\").get_text().strip(),\n        \"path\": path,\n    }\n    node.update(attr_values(navpoint.attrs))\n    child_path = path + [node[\"label\"]]\n    nodes = [node]\n    for child_np in navpoint.find_all(\"navpoint\", recursive=False):\n        child_nodes = process_navpoint(child_np, child_path)\n        nodes.extend(child_nodes)\n    return nodes\n\ndef attr_values(attrs):\n    \"Book-specific interpretation of TOC attributes\"\n    vals = {\n        \"class\": attrs[\"class\"][0],\n        \"id\": attrs[\"id\"],\n        \"playorder\": attrs[\"playorder\"],\n    }\n    return vals\n\ndef chapter_contents(item, node):\n    chapter_chunks = []\n    soup = BeautifulSoup(item.get_body_content(), \"html.parser\")\n    # Iterate over every tag\n    page = \"-\"\n    if soup.div:\n        root_tag = soup.div\n    else:\n        root_tag = soup\n    for tag in root_tag.find_all(True, recursive=False):\n        if ((tag.name == \"a\") and\n            (\"id\" in tag.attrs) and\n            tag[\"id\"].startswith(\"page\")):\n            page = tag[\"id\"]\n        else:\n            chunk = {\n                \"text\": tag.get_text().strip(),\n                \"page\": page,\n            }\n            chunk.update(node)\n            chapter_chunks.append(chunk)\n    return chapter_chunks\n\ndef coalesce_pages(chunks):\n    \"\"\"Combine the texts of items that share the same page.\"\"\"\n    keys = [\"page\"]\n    key_func = itemgetter(*keys)\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size = MAX_CHUNK_CHARS,\n        chunk_overlap = 0,\n        separators = [\".\\n\", \"\\n\\n\", \"\\r\\n\", \"\\n\"], \n    )\n    page_chunks = []\n    for page_key, page_iter in groupby(chunks, key_func):\n        chunk_nodes = [pi for pi in page_iter]\n        page_chunk_proto = chunk_nodes[0]\n        page_text = \"\\n\".join(pi[\"text\"] for pi in chunk_nodes)\n        page_texts = text_splitter.split_text(page_text)\n        for text in page_texts:\n            if text:\n                page_chunk = page_chunk_proto.copy()\n                page_chunk[\"text\"] = text\n                page_chunks.append(page_chunk)\n    return page_chunks\n    \ndef path_to_volume_book(path):\n    match path:\n        case []:\n            vol_book = None\n        case [volume]:\n            vol_book = {\"volume\": volume.title()}\n        case [volume, book]:\n            vol_book = {\n                \"volume\": volume.title(),\n                \"book\": book.title(),\n            }\n    return vol_book\n\nif __name__ == \"__main__\":\n    main()\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}